{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbae310",
   "metadata": {},
   "source": [
    "\n",
    "# Q-2 — LLM Fine-Tuning (RLHF‑Lite) with Raw Student Answers\n",
    "\n",
    "This notebook builds an **end-to-end** pipeline for Q‑2 using raw student answers from `midterm_1-q2.txt`.  \n",
    "It includes:\n",
    "1) Data loading & cleaning (raw text, no stopword removal)  \n",
    "2) Baseline semantic scoring using multiple models (SBERT, BERTScore, optional T5/ROUGE)  \n",
    "3) Reward model construction (ensemble of scorers)  \n",
    "4) **RLHF‑Lite** fine‑tuning of a small causal LM (DistilGPT‑2) using a reward‑weighted loss  \n",
    "5) Evaluation against the teacher reference answer (before vs after fine‑tuning)  \n",
    "6) Transparent selection and display of **5 human reference samples** (very_low → very_high)  \n",
    "7) Optional strict‑zero & damping rules for exam policy alignment  \n",
    "\n",
    "> This notebook is modular: you can swap/distil models or add your own scorers.  \n",
    "> Run each section in order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3e099",
   "metadata": {},
   "source": [
    "## 0) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc67084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Teacher reference answer (Q-2)\n",
    "TEACHER_ANSWER = 'The Roman Empire developed comprehensive legal codes and principles that form the basis of Western legal systems. Roman law, particularly in the areas of property rights, contractual obligations, inheritance laws, and criminal law, has left a lasting impact. Known as the \"Law of Laws,\" Roman Law, including the Justinian Codes and the compilation of Roman legal thought in the Corpus Juris Civilis, remains a pivotal influence in legal theory and practice today.'\n",
    "\n",
    "# File paths\n",
    "RAW_TXT = \"midterm_1-q2.txt\"   # format: StudentID:::Answer\n",
    "OUTPUT_DIR = \"rlhf_q2_outputs\"\n",
    "\n",
    "# Policy knobs\n",
    "STRICT_ZERO = True            # zero for empty or trivial 'copy'\n",
    "DAMP_LOW_SCORES = True        # if True, <=5 human scores gain at most +5\n",
    "MAX_DAMP_GAIN = 5\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9753d",
   "metadata": {},
   "source": [
    "## 1) Setup (install & imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aceb1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bkkas/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/bkkas/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F6236B89-E4CA-3330-B665-E463D537EAF3> /Users/bkkas/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <552B36CA-07A6-332B-BF7F-6D22D9005F71> /Users/bkkas/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You may need internet to install these in your own environment.\n",
    "# If they're already available, you can skip installation cells.\n",
    "\n",
    "%pip -q install transformers datasets sentence-transformers bert-score evaluate accelerate einops --upgrade\n",
    "\n",
    "import os, math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util as st_util\n",
    "from bert_score import score as bert_score\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38223006",
   "metadata": {},
   "source": [
    "## 2) Load raw student answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e418f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 answers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StudentID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b274d0b3-dbbb-4eee-a96f-79630788315c",
       "rows": [
        [
         "0",
         "20180808086",
         "Romans achievements are in a Modern Goverment. Consult,senate and Assemblies. the other achievements about science,enginnering is Romans archs,Newspaper, The 12 table, julian Calander, Concrete.   They differ from Greeks; Greeks was having great open air setting and allowed everyone watch.And greeks are achievements Democrasy."
        ],
        [
         "1",
         "20190808014",
         "Romans advanced in modern legal system, civil engineering, military engineering and more like gregorian calendar. They also divided powers, known as the divison of the powers. There were 3 parties: Conculs, Senates and Assemblies. I also must mention about conflict of interests, I can give that example: Senators were forbidden to make trade, because they would use their influences over their personal gains of trading. They differ in terms of theaters, while Greeks was having great open-air settings, and allowed everyone to watch, and were about mostly tales, gods, love etc, Romanian theater was more about personal stories, closed and realistic shows. Greeks also acted with maskes sometimes. And theater haved influence over democracy."
        ],
        [
         "2",
         "20190808021",
         "The ten main achievements of romans were the battlefield surgeries, bound books, the 12 tables, the julian calendar, aqueducts, roman bridges, roads and highways, newspapers, welfares and concrete.   The Greeks were mostly working on mathematical, philosophical and astronomical improvements. While the Romans worked on military, building and law improvements. Thus we can say Greeks were more of a thinker while Romans were more of a builder."
        ],
        [
         "3",
         "20190808035",
         "Roads and highways   using geometry   military engineering   they use the science to make changes in their life"
        ],
        [
         "4",
         "20200808008",
         "Romans achieved civil and army engineering, rebuplic government system, fundamentals of modern legal systems, a road and trade web all across the europe, they rule Medittarenean for hundereds of years. Greeks are seeking for answers to questions about nature, they are developing theories and philosophies but Romans did not developed them. Romans used what science and philosophy they already have and improved their tools, armies, cities, laws and goverment system that divise the power of rulars."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180808086</td>\n",
       "      <td>Romans achievements are in a Modern Goverment....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190808014</td>\n",
       "      <td>Romans advanced in modern legal system, civil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190808021</td>\n",
       "      <td>The ten main achievements of romans were the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190808035</td>\n",
       "      <td>Roads and highways   using geometry   military...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200808008</td>\n",
       "      <td>Romans achieved civil and army engineering, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     StudentID                                             Answer\n",
       "0  20180808086  Romans achievements are in a Modern Goverment....\n",
       "1  20190808014  Romans advanced in modern legal system, civil ...\n",
       "2  20190808021  The ten main achievements of romans were the b...\n",
       "3  20190808035  Roads and highways   using geometry   military...\n",
       "4  20200808008  Romans achieved civil and army engineering, re..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_raw_txt(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line or \":::\" not in line:\n",
    "                continue\n",
    "            sid, ans = line.split(\":::\", 1)\n",
    "            rows.append({\"StudentID\": sid.strip(), \"Answer\": ans.strip()})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_raw = load_raw_txt(RAW_TXT)\n",
    "# Drop empty purely\n",
    "df_raw = df_raw[df_raw[\"Answer\"].str.strip()!=\"\"].copy().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df_raw)} answers.\")\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e2d4e",
   "metadata": {},
   "source": [
    "## 3) Baseline scoring with multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8ec5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSBERT_Sim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(sbert_sim)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 3.2 BERTScore (precision-oriented, can choose F1 too)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m bert_score([df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()], [ [TEACHER_ANSWER]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(df) ], lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, rescale_with_baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore_F1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m F1]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 3.3 ROUGE (optional lexical baseline)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bert_score/score.py:123\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalculating scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 123\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m bert_cos_score_idf(\n\u001b[1;32m    124\u001b[0m     model,\n\u001b[1;32m    125\u001b[0m     refs,\n\u001b[1;32m    126\u001b[0m     cands,\n\u001b[1;32m    127\u001b[0m     tokenizer,\n\u001b[1;32m    128\u001b[0m     idf_dict,\n\u001b[1;32m    129\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    130\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    131\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    132\u001b[0m     all_layers\u001b[38;5;241m=\u001b[39mall_layers,\n\u001b[1;32m    133\u001b[0m )\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref_group_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     max_preds \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bert_score/utils.py:607\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 607\u001b[0m sentences \u001b[38;5;241m=\u001b[39m dedup_and_sort(refs \u001b[38;5;241m+\u001b[39m hyps)\n\u001b[1;32m    608\u001b[0m embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    609\u001b[0m iter_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bert_score/utils.py:605\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3.1 Sentence-BERT semantic similarity\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "teacher_emb = sbert.encode(TEACHER_ANSWER, convert_to_tensor=True)\n",
    "\n",
    "def sbert_sim(text):\n",
    "    emb = sbert.encode(text, convert_to_tensor=True)\n",
    "    return st_util.cos_sim(emb, teacher_emb).item()\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"SBERT_Sim\"] = df[\"Answer\"].apply(sbert_sim)\n",
    "\n",
    "# 3.2 BERTScore (precision-oriented, can choose F1 too)\n",
    "P, R, F1 = bert_score([df[\"Answer\"].tolist()], [ [TEACHER_ANSWER]*len(df) ], lang=\"en\", rescale_with_baseline=True)\n",
    "df[\"BERTScore_F1\"] = [float(x) for x in F1]\n",
    "\n",
    "# 3.3 ROUGE (optional lexical baseline)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_scores = rouge.compute(predictions=df[\"Answer\"].tolist(), references=[TEACHER_ANSWER]*len(df))\n",
    "# ROUGE returns corpus-level. We'll also compute per-sample ROUGE-L by batching:\n",
    "def rougeL_single(pred, ref):\n",
    "    sc = rouge.compute(predictions=[pred], references=[ref])\n",
    "    return sc[\"rougeL\"]\n",
    "\n",
    "df[\"ROUGE_L\"] = df[\"Answer\"].apply(lambda x: rougeL_single(x, TEACHER_ANSWER))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8790154",
   "metadata": {},
   "source": [
    "## 4) Build reward model (ensemble of scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e008ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize each metric to [0,1] via MinMax\n",
    "for col in [\"SBERT_Sim\", \"BERTScore_F1\", \"ROUGE_L\"]:\n",
    "    scaler = MinMaxScaler()\n",
    "    df[col+\"_N\"] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "# Simple linear ensemble; you can tune weights\n",
    "df[\"Reward_Ensemble\"] = 0.5*df[\"SBERT_Sim_N\"] + 0.4*df[\"BERTScore_F1_N\"] + 0.1*df[\"ROUGE_L_N\"]\n",
    "\n",
    "# Optional: learn a tiny ridge regressor on pseudo-targets if you have human scores\n",
    "# For Q-2 we assume only teacher ref; so keep ensemble as reward\n",
    "df[[\"SBERT_Sim\",\"BERTScore_F1\",\"ROUGE_L\",\"Reward_Ensemble\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0ac7b",
   "metadata": {},
   "source": [
    "## 5) Pick 5 human reference samples (very_low → very_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We ensure exactly 5 bins are represented.\n",
    "bins = [df[\"Reward_Ensemble\"].min()-1e-9, 0.2, 0.4, 0.6, 0.8, df[\"Reward_Ensemble\"].max()+1e-9]\n",
    "labels = [\"very_low\",\"low\",\"medium\",\"high\",\"very_high\"]\n",
    "df[\"level\"] = pd.cut(df[\"Reward_Ensemble\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "rep_list = []\n",
    "for lab in labels:\n",
    "    sub = df[df[\"level\"]==lab]\n",
    "    if sub.empty:\n",
    "        # backoff: take nearest neighbors from adjacent bins\n",
    "        # if really no sample, duplicate the closest from next non-empty bin\n",
    "        # (keeps 5 representatives always)\n",
    "        continue\n",
    "    rep_list.append(sub.sample(1, random_state=RANDOM_SEED))\n",
    "rep_df = pd.concat(rep_list).reset_index(drop=True)\n",
    "\n",
    "# If fewer than 5, fill from global extremes/middle\n",
    "while len(rep_df) < 5:\n",
    "    # pick from remaining by distance to target quantiles\n",
    "    q_targets = [0.1,0.3,0.5,0.7,0.9]\n",
    "    target = q_targets[len(rep_df)%5]\n",
    "    tgt_val = df[\"Reward_Ensemble\"].quantile(target)\n",
    "    pick = df.iloc[(df[\"Reward_Ensemble\"]-tgt_val).abs().argsort().values[0]]\n",
    "    if pick[\"StudentID\"] not in rep_df[\"StudentID\"].tolist():\n",
    "        rep_df = pd.concat([rep_df, pd.DataFrame([pick])], ignore_index=True)\n",
    "\n",
    "rep_df = rep_df.drop_duplicates(subset=[\"StudentID\"]).head(5)\n",
    "rep_df[[\"StudentID\",\"Reward_Ensemble\",\"level\",\"Answer\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19576c47",
   "metadata": {},
   "source": [
    "## 6) Prepare dataset for RLHF‑Lite fine‑tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize answers; reward becomes per-sample weight\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_with_reward(ex):\n",
    "    toks = tokenizer(\n",
    "        ex[\"Answer\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    toks = {k: v.squeeze(0) for k,v in toks.items()}\n",
    "    toks[\"labels\"] = toks[\"input_ids\"].clone()\n",
    "    # attach reward as tensor\n",
    "    toks[\"reward\"] = torch.tensor(ex[\"Reward_Ensemble\"], dtype=torch.float32)\n",
    "    return toks\n",
    "\n",
    "train_ds = Dataset.from_pandas(df[[\"Answer\",\"Reward_Ensemble\"]].copy())\n",
    "train_ds = train_ds.map(tokenize_with_reward)\n",
    "\n",
    "train_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6adab",
   "metadata": {},
   "source": [
    "## 7) Fine‑tune causal LM with reward‑weighted loss (RLHF‑Lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards = inputs.pop(\"reward\")  # shape: (B,)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  # (B,) or scalar\n",
    "        if loss.ndim == 0:\n",
    "            loss = loss * (1.0 + rewards.mean())\n",
    "        else:\n",
    "            loss = (loss * (1.0 + rewards)).mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/rlhf_q2_ckpt\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/rlhf_q2_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/rlhf_q2_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d29d8e",
   "metadata": {},
   "source": [
    "## 8) Evaluation: before vs after fine‑tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ada349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer(prompt, mdl, tok, max_new_tokens=80):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(mdl.device) for k,v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = mdl.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.95, top_k=50)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Use the same student answers as prompts or a question template\n",
    "PROMPT_PREFIX = \"Q-2: Explain the lasting impact of Roman Law on Western legal systems. Student answer:\\n\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(f\"{OUTPUT_DIR}/rlhf_q2_model\")\n",
    "\n",
    "eval_samples = df.sample(min(20, len(df)), random_state=RANDOM_SEED).copy()\n",
    "\n",
    "def eval_model(mdl, name):\n",
    "    outs = []\n",
    "    for _, row in eval_samples.iterrows():\n",
    "        prompt = PROMPT_PREFIX + row[\"Answer\"] + \"\\nRefine:\"\n",
    "        gen = generate_answer(prompt, mdl, tokenizer, max_new_tokens=80)\n",
    "        outs.append(gen)\n",
    "    eval_samples[name] = outs\n",
    "\n",
    "eval_model(base_model, \"gen_base\")\n",
    "eval_model(finetuned_model, \"gen_finetuned\")\n",
    "\n",
    "# Score generations against teacher reference\n",
    "def batch_bertscore(preds, ref):\n",
    "    P, R, F1 = bert_score(preds, [ref]*len(preds), lang=\"en\", rescale_with_baseline=True)\n",
    "    return np.array([float(x) for x in F1])\n",
    "\n",
    "eval_samples[\"BS_base\"] = batch_bertscore(eval_samples[\"gen_base\"].tolist(), TEACHER_ANSWER)\n",
    "eval_samples[\"BS_ft\"]   = batch_bertscore(eval_samples[\"gen_finetuned\"].tolist(), TEACHER_ANSWER)\n",
    "\n",
    "gain = (eval_samples[\"BS_ft\"] - eval_samples[\"BS_base\"]).mean()\n",
    "print(f\"Avg BERTScore F1 gain (finetuned - base): {gain:.4f}\")\n",
    "eval_samples.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942fbf2",
   "metadata": {},
   "source": [
    "## 9) Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd21ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_xlsx = Path(OUTPUT_DIR) / \"rlhf_q2_results.xlsx\"\n",
    "with pd.ExcelWriter(out_xlsx) as writer:\n",
    "    df.to_excel(writer, sheet_name=\"Raw + Scores + Reward\", index=False)\n",
    "    rep_df[[\"StudentID\",\"Reward_Ensemble\",\"level\",\"Answer\"]].to_excel(writer, sheet_name=\"Human Reference (5)\", index=False)\n",
    "    eval_samples.to_excel(writer, sheet_name=\"Gen Eval (Base vs FT)\", index=False)\n",
    "\n",
    "print(\"Saved:\", out_xlsx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
